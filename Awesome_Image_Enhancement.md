# Awesome Image Enhancement

| Year |	Proceeding	| Title | PDF | Code |
| :---: | :---: | :---: | :---: | :---: |
| 2022 | CVPR | MAXIM: Multi-Axis MLP for Image Processing | [Click Here](https://arxiv.org/pdf/2201.02973.pdf) | [Click Here](https://github.com/google-research/maxim) |
| 2022 | CVPR | Abandoning the Bayer-Filter to See in the Dark | [Click Here](https://arxiv.org/pdf/2203.04042.pdf) | [Click Here](https://github.com/TCL-AILab/Abandon_Bayer-Filter_See_in_the_Dark) |
| 2022 | CVPR | URetinex-Net: Retinex-based Deep Unfolding Network for Low-light-Image-Enhancement | [Click Here](https://openaccess.thecvf.com/content/CVPR2022/papers/Wu_URetinex-Net_Retinex-Based_Deep_Unfolding_Network_for_Low-Light_Image_Enhancement_CVPR_2022_paper.pdf) | [Click Here](https://github.com/AndersonYong/URetinex-Net) |
| 2022 | CVPR | Toward Fast, Flexible, and Robust Low-Light Image Enhancement | [Click Here](https://arxiv.org/pdf/2204.10137.pdf) | [Click Here](https://github.com/vis-opt-group/SCI) |
| 2021 | CVPR | Retinex-inspired Unrolling with Cooperative Prior Architecture Search for Low-light Image Enhancement | [Click Here](https://arxiv.org/pdf/2012.05609.pdf) | [Click Here](https://github.com/KarelZhang/RUAS) || 2020 | CVPR | Learning to Restore Low-Light Images via Decomposition-and-Enhancement | [Click Here](https://openaccess.thecvf.com/content_CVPR_2020/papers/Xu_Learning_to_Restore_Low-Light_Images_via_Decomposition-and-Enhancement_CVPR_2020_paper.pdf) | [Click Here](https://drive.google.com/drive/folders/1L3RDbd3sk_TcMTrSmZXn8KLg8opjOjf0) |
| 2021 | CVPR | Learning Temporal Consistency for Low Light Video Enhancement from Single Images | [Click Here](https://openaccess.thecvf.com/content_CVPR_2020/papers/Xu_Learning_to_Restore_Low-Light_Images_via_Decomposition-and-Enhancement_CVPR_2020_paper.pdf) | [Click Here](https://github.com/zkawfanx/StableLLVE) |
| 2020 | CVPR | From Fidelity to Perceptual Quality: A Semi-Supervised Approach for Low-Light Image Enhancement | [Click Here](https://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_From_Fidelity_to_Perceptual_Quality_A_Semi-Supervised_Approach_for_Low-Light_CVPR_2020_paper.pdf) | [Click Here](https://github.com/flyywh/CVPR-2020-Semi-Low-Light) |

| 2022 | ICPR | [DocEnTr: An End-to-End Document Image Enhancement Transformer](#docentr-an-end-to-end-document-image-enhancement-transformer-icpr2022) | [Click Here](https://arxiv.org/pdf/2201.10252.pdf) | [Click Here](https://github.com/dali92002/DocEnTR)
| 2023 | MDPI | Unsupervised Low Light Image Enhancement Transformer Based on Dual Contrastive Learning | [Click Here](https://bmvc2022.mpi-inf.mpg.de/0373.pdf) | [Click Here](https://github.com/KaedeKK/UDCL-Transformer)
| 2023 | MDPI | Low-Light Image Enhancement by Combining Transformer and Convolutional Neural Network | [Click Here](https://www.mdpi.com/2227-7390/11/7/1657/pdf?version=1680159122) | [Click Here]
| 2022 | CVPR | [SNR-aware Low-Light Image Enhancement](#snr-aware-low-light-image-enhancement-cvpr2022) | [Click Here](https://openaccess.thecvf.com/content/CVPR2022/papers/Xu_SNR-Aware_Low-Light_Image_Enhancement_CVPR_2022_paper.pdf) | [Click Here](https://github.com/dvlab-research/SNR-Aware-Low-Light-Enhance) |
| 2022 | BMVC | [You Only Need 90K Parameters to Adapt Light: A Light Weight Transformer for Image Enhancement and Exposure Correction](#you-only-need-90k-parameters-to-adapt-light-a-light-weight-transformer-for-image-enhancement-and-exposure-correction-bmvc2022) | [Click Here](https://arxiv.org/pdf/2205.14871) | [Click Here](https://github.com/cuiziteng/Illumination-Adaptive-Transformer) |
| 2021 | ICCV | [STAR: A Structure-aware Lightweight Transformer for Real-time Image Enhancement](star-a-structure-aware-lightweight-transformer-for-real-time-image-enhancement-iccv2021) | [Click Here](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_STAR_A_Structure-Aware_Lightweight_Transformer_for_Real-Time_Image_Enhancement_ICCV_2021_paper.pdf) | [Click Here](https://github.com/zzyfd/STAR-pytorch) |
| 2023 | Arxiv | [Self-Reference Deep Adaptive Curve Estimation for Low-Light Image Enhancement](#self-reference-deep-adaptive-curve-estimation-for-low-light-image-enhancement) | [Click Here](https://arxiv.org/pdf/2308.08197v2.pdf) | [Click Here](https://github.com/john-venti/self-dace) |
| 2023 | CVPR | [Learning a Simple Low-light Image Enhancer from Paired Low-light Instances](#learning-a-simple-low-light-image-enhancer-from-paired-low-light-instances-cvpr2023) | [Click Here](https://openaccess.thecvf.com/content/CVPR2023/papers/Fu_Learning_a_Simple_Low-Light_Image_Enhancer_From_Paired_Low-Light_Instances_CVPR_2023_paper.pdf) | [Click Here](https://github.com/zhenqifu/pairlie) |
| 2022 | CoRR | Low-Light Image and Video Enhancement: A Comprehensive Survey and Beyond | [Click Here](https://arxiv.org/pdf/2212.10772.pdf) | [Click Here] |
| 2021 | Springer | Benchmarking Low-Light Image Enhancement and Beyond | [Click Here](https://sci-hub.se/10.1007/s11263-020-01418-8) | [Click Here] |
| 2021 | IEEE Trans | Low-Light Image and Video Enhancement Using Deep Learning: A Survey | [Click Here](https://arxiv.org/pdf/2104.10729) | [Click Here] |
| 2022 | IEEE Trans | Underwater Image Enhancement via Minimal Color Loss and Locally Adaptive Contrast Enhancement | [Click Here](https://drive.google.com/file/d/1d8gLKDPoxISy6-oVc6bQJZ-sElmNDPbB/view) | [Click Here]() |
| 2022 | IEEE Trans(TPAMI ) | [Learning Enriched Features for Fast Image Restoration and Enhancement](#learning-enriched-features-for-fast-image-restoration-and-enhancement-ieee2022) | [Click Here](https://arxiv.org/pdf/2205.01649) | [Click Here](https://github.com/swz30/MIRNetv2) |
| 2020 | ECCV | Learning Enriched Features for Real Image Restoration and Enhancement | [Click Here](https://arxiv.org/pdf/2003.06792v2.pdf) | [Click Here](https://github.com/swz30/MIRNet) |
| 2020 | CVPR | [Zero-Reference Deep Curve Estimation for Low-Light Image Enhancement](#zero-reference-deep-curve-estimation-for-low-light-image-enhancement-cvpr2020) | [Click Here](https://openaccess.thecvf.com/content_CVPR_2020/papers/Guo_Zero-Reference_Deep_Curve_Estimation_for_Low-Light_Image_Enhancement_CVPR_2020_paper.pdf) | [Click Here](https://github.com/Li-Chongyi/Zero-DCE) |
| 2021 | IEEE Trans | underwater image enhancement via medium transmission-guided multi-color space embedding | [Click Here](https://arxiv.org/pdf/2104.13015.pdf) | [Click Here](https://github.com/Li-Chongyi/Ucolor) |
| 2023 | CVPR | [Burstormer: Burst Image Restoration and Enhancement Transformer](#burstormer-burst-image-restoration-and-enhancement-transformer-cvpr2023) | [Click Here](https://openaccess.thecvf.com/content/CVPR2023/papers/Dudhane_Burstormer_Burst_Image_Restoration_and_Enhancement_Transformer_CVPR_2023_paper.pdf) | [Click Here](https://github.com/akshaydudhane16/Burstormer) |
| 2023 | AAAI  | [Ultra-High-Definition Low-Light Image Enhancement: A Benchmark and Transformer-Based Method](#ultra-high-definition-low-light-image-inhancement-a-benchmark-and-transformer-based-method-aaai2023) | [Click Here](https://arxiv.org/pdf/2212.11548) | [Click Here](https://github.com/TaoWangzj/LLFormer) |
| 2022 | IEEE Trans | U-shape Transformer for Underwater Image Enhancement | [Click Here](https://arxiv.org/pdf/2111.11843) | [Click Here](https://github.com/LintaoPeng/U-shape_Transformer_for_Underwater_Image_Enhancement) |
| 2022 | IEEE Trans | twin adversarial contrastive learning for underwater image enhancement and beyond | [Click Here](https://drive.google.com/file/d/1rAQVj1RamqHI0OefUTnAeqV6vNFdAm0S/view) | [Click Here](https://github.com/Jzy2017/TACL) |
|  |  |  | [Click Here]() | [Click Here]() |

<br />
## Learning a Simple Low-light Image Enhancer from Paired Low-light Instances CVPR2023

**Purpose:** The purpose of this paper is to introduce PairLIE, an unsupervised approach for low-light image enhancement that learns adaptive priors from low-light image pairs. The paper aims to improve contrast and restore details for images captured in low-light conditions.

**Method:** PairLIE is an unsupervised approach for low-light image enhancement that learns adaptive priors from low-light image pairs. The method consists of two main steps:

1. Retinex Decomposition: The network is expected to generate the same clean images as the two inputs share the same image content. To achieve this, the network is imposed with the Retinex theory and makes the two reflectance components consistent.
2. Self-Supervised Mechanism: To assist the Retinex decomposition, inappropriate features in the raw image are removed with a simple self-supervised mechanism.

**Datasets:**
* SICE (part2)
*  LOL (training set)

**Hardware:** Not mentioned

<p align="center">
  <img src="https://github.com/farkoo/AbstractVault/blob/master/IE_PairLIE_arch.png" alt="PairLIE Architecture">
  <br>
  <em>PairLIE Architecture</em>
</p>

<p align="center">
  <img src="https://github.com/farkoo/AbstractVault/blob/master/IE_PairLIE_result.png" alt="PairLIE Architecture">
  <br>
  <em>PairLIE Results</em>
</p>

<br />
## Burstormer Burst Image Restoration and Enhancement Transformer CVPR2023

**Method:** Burstormer is a novel transformer-based architecture for burst image restoration and enhancement. It leverages multi-scale local and non-local features to achieve improved alignment and feature fusion. The proposed method consists of two main parts: enhanced deformable alignment (EDA) and image reconstruction. EDA is a multi-scale hierarchical module that extracts noise-free local and non-local features with the burst feature attention (BFA), performs feature alignment, and refines and consolidates features with an additional interaction with the base frame via the proposed reference-based feature enrichment (RBFE) module. The input burst frames need to be properly aligned before fusing their information. Therefore, the proposed alignment module not only aligns burst features but also exchanges feature information and maintains focused communication with the reference frame through the proposed reference-based feature enrichment mechanism, which facilitates handling complex motions. Burstormer sets new state-of-the-art on several real and synthetic benchmark datasets for the task of *burst super-resolution*, *burst low-light enhancement*, and *burst denoising*.

**Dataset:**
* SyntheticBurst
* BurstSR
* SID
* Grayscale burst denoising on the dataset by [30]
* Color burst denoising on the dataset by [41]
  
**Hardware:**  Four RTX6000 GPU

<p align="center">
  <img src="https://github.com/farkoo/AbstractVault/blob/master/IE_Burstormer_Arch.png" alt="Burstormer Architecture">
  <br>
  <em>Burstormer Architecture</em>
</p>

<p align="center">
  <img src="https://github.com/farkoo/AbstractVault/blob/master/IE_Burstormer_SR.png" alt="Burstormer Super Resolution">
  <br>
  <em>Burstormer Super Resolution</em>
</p>

<p align="center">
  <img src="https://github.com/farkoo/AbstractVault/blob/master/IE_Burstormer_LLIE.png" alt="Burstormer Low-Light Image Enhancement">
  <br>
  <em>Burstormer Low-Light Image Enhancement</em>
</p>

<p align="center">
  <img src="https://github.com/farkoo/AbstractVault/blob/master/IE_Burstormer_DN.png" alt="Grayscale and Color Burst Denoising">
  <br>
  <em>Grayscale and Color Burst Denoising</em>
</p>

<br />
## SNR-Aware Low-light Image Enhancement CVPR2022

**Method:** The SNR-Aware Low-light Image Enhancement method uses a signal-to-noise-aware framework consisting of a new SNR-aware transformer design and a convolutional model to adaptively enhance low-light images in a spatial-varying manner. The method first obtains an SNR map using a simple and yet effective strategy, which estimates the Signal-to-Noise Ratio (SNR) of the input low-light image. The SNR map is then used to guide the framework to learn different enhancement operations adaptively for image regions of varying signal-to-noise ratios. In the deepest hidden layer of the framework, a new self-attention module is used to enhance pixels in a spatial-varying manner and avoid inaccurate information from regions of very low SNR. The method has been shown to achieve superior perceptual quality and consistently outperform other approaches on seven benchmarks.

**Dataset:**
* LOL
* SID
* SMID
* SDSD

**Hardware:** It train and test it on a PC with a 2080Ti GPU

<p align="center">
  <img src="https://github.com/farkoo/AbstractVault/blob/master/IE_SNRAT_Arch.png" alt="SNR-Aware Low-light Image Enhancement Architecture">
  <br>
  <em>SNR-Aware Low-light Image Enhancement Architecture</em>
</p>

<p align="center">
  <img src="https://github.com/farkoo/AbstractVault/blob/master/IE_SNRAT_Result.png" alt="SNR-Aware Low-light Image Enhancement Results">
  <br>
  <em>SNR-Aware Low-light Image Enhancement Results</em>
</p>

<br />
## MAXIM: Multi-Axis MLP for Image Processing CVPR2022

<br />
## Abandoning the Bayer-Filter to See in the Dark CVPR2022

<br />
## URetinex-Net: Retinex-based Deep Unfolding Network for Low-light-Image-Enhancement CVPR2022

<br />
## Toward Fast, Flexible, and Robust Low-Light Image Enhancement CVPR2022

<br />
## Retinex-inspired Unrolling with Cooperative Prior Architecture Search for Low-light Image Enhancement CVPR2021

<br />
## Learning Temporal Consistency for Low Light Video Enhancement from Single Images CVPR2021

<br />
## From Fidelity to Perceptual Quality: A Semi-Supervised Approach for Low-Light Image Enhancement CVPR 2020

<br />
## Zero-Reference Deep Curve Estimation for Low-Light Image Enhancement CVPR2020

**Methos:** It does not require any paired or unpaired data in the training process, and it is trained without any reference image. Instead of performing image-to-image mapping, the task is reformulated as an image-specific curve estimation problem. The proposed method takes a low-light image as input and produces high-order curves as its output. These curves are then used for pixel-wise adjustment on the dynamic range of the input to obtain an enhanced image. The curve estimation is carefully formulated so that it maintains the range of the enhanced image and preserves the contrast of neighboring pixels.

**Datasets:** 360 multi-exposure sequences from the Part1 of SICE dataset

**Hardware:** One NVIDIA 2080Ti GPU

<p align="center">
  <img src="https://github.com/farkoo/AbstractVault/blob/master/IE_Zero_DCE_Arch.png" alt="Zero-DCE Architecture">
  <br>
  <em>Zero-DCE Architecture</em>
</p>

<p align="center">
  <img src="https://github.com/farkoo/AbstractVault/blob/master/IE_Zero_DCE_Result.png" alt="Zero-DCE Results">
  <br>
  <em>Zero-DCE Results</em>
</p>

<br>
## Self-Reference Deep Adaptive Curve Estimation for Low-Light Image Enhancement

**Method:** Self-DACE (Self-Reference Deep Adaptive Curve Estimation) is a 2-stage low-light image enhancement method proposed in this paper. The method is inspired by the ZeroDCE algorithm and employs a more flexible class of adaptive adjustment curves iteratively to enhance low-light images with a wider dynamic range. The first stage of the method estimates the initial enhancement curve using a self-reference mechanism, while the second stage refines the curve using a denoising scheme. The proposed method is shown to outperform existing state-of-the-art algorithms in terms of quantitative metrics on multiple datasets.

**Dataset:**
* LOL
* LSRW
* SICE
  
**Hardware:** One NVIDIA RTX 3080-TiGPU

<p align="center">
  <img src="https://github.com/farkoo/AbstractVault/blob/master/IE_Self_DICE_Arch.png" alt="Self-DICE Architecture">
  <br>
  <em>Self-DICE Architecture</em>
</p>

<p align="center">
  <img src="https://github.com/farkoo/AbstractVault/blob/master/IE_Self_DICE_Result.png" alt="Self-DICE Results">
  <br>
  <em>Self-DICE Results</em>
</p>

<br />
## Learning Enriched Features for Fast Image Restoration and Enhancement IEEE2022

**Method:** The method presented in this paper is a new architecture for fast image restoration and enhancement. The proposed architecture is based on convolutional neural networks (CNNs) and is designed to maintain spatially-precise high-resolution representations while receiving complementary contextual information from low-resolution representations. 

The architecture consists of a multi-scale residual block that processes the input image at different scales and combines the features learned at each scale to produce the final output. The multi-scale residual block is augmented with a non-local attention mechanism that helps capture contextual information from the low-resolution representations. 

The proposed architecture is evaluated on several image restoration tasks, including image denoising, super-resolution, and JPEG artifact removal. The experimental results show that the proposed approach outperforms several state-of-the-art methods on these tasks in terms of both quantitative metrics and visual quality.

**Dataset:** 
* Dual-pixel defocus deblurring (DPDD)
* DND (Denoising)
* SIDD (Denoising)
* RealSR (Super Resolution)
* LOL (Image Enhancement)
* MIT-Adobe FiveK (Image Enhancement)
  
**Hardware:** Not Mentioned

<p align="center">
  <img src="https://github.com/farkoo/AbstractVault/blob/master/IE_MIRNet-v2_Arch.png" alt="MIRNet-v2 Architecture">
  <br>
  <em>MIRNet-v2 Architecture</em>
</p>

<p align="center">
  <img src="https://github.com/farkoo/AbstractVault/blob/master/IE_MIRNet-v2_Deblurring.png" alt="MIRNet-v2 Deblurring">
  <br>
  <em>MIRNet-v2 Deblurring</em>
</p>

<p align="center">
  <img src="https://github.com/farkoo/AbstractVault/blob/master/IE_MIRNet-v2_Deblurring.png" alt="MIRNet-v2 Denoising">
  <br>
  <em>MIRNet-v2 Denoising</em>
</p>

<p align="center">
  <img src="https://github.com/farkoo/AbstractVault/blob/master/IE_MIRNet-v2_LLIE.png" alt="MIRNet-v2 Low-Light Image Enhancement">
  <br>
  <em>MIRNet-v2 Low-Light Image Enhancement</em>
</p>

<p align="center">
  <img src="https://github.com/farkoo/AbstractVault/blob/master/IE_MIRNet-v2_SR.png" alt="MIRNet-v2 Super Resolution">
  <br>
  <em>MIRNet-v2 Super Resolution</em>
</p>

<br />
## Ultra-High-Definition Low-Light Image Enhancement A Benchmark and Transformer-Based Method AAAI2023

**Method:** LLFormer is a transformer-based low-light enhancement method introduced in this PDF file. It uses axis-based multi-head self-attention and cross-layer attention fusion blocks to significantly reduce linear complexity and improve performance on low-light images. LLFormer was benchmarked against 16 representative LLIE methods, including seven traditional non-learning methods, and was found to outperform state-of-the-art methods on both a new large-scale database and existing public datasets.

**Dataset:**
* UHD-LOL4K
* UHD-LOL8K
* LOL
* MIT-Adobe FiveK

**Hardware:** Not Mentioned

<p align="center">
  <img src="https://github.com/farkoo/AbstractVault/blob/master/IE_LLFormer_Arch.png" alt="LLFormer Architecture">
  <br>
  <em>LLFormer Architecture</em>
</p>

<p align="center">
  <img src="https://github.com/farkoo/AbstractVault/blob/master/IE_LLFormer_Result.png" alt="LLFormer Results">
  <br>
  <em>LLFormer Results</em>
</p>

<br>
## STAR A Structure-aware Lightweight Transformer for Real-time Image Enhancement ICCV2021

**Method:** The article presents a method called STAR, which is a structure-aware lightweight Transformer for real-time image enhancement. It tokenizes image patches into token embeddings and explicitly learns token-wise dependencies for image patches. STAR is specialized in image enhancement tasks and can be free of stacked convolution, making it more efficient in extracting structural information. It employs a specialized two-branch design named long-short Range Transformer to ensure it can focus on capturing global contexts, thus reducing computations. Experimental results show that STAR can effectively boost the quality and efficiency of many tasks such as illumination enhancement, auto white balance, and photo retouching.

**Dataset:** 
* MIT-Adobe FiveK (Illumination Enhancement)
* WB dataset (White Balance)
* Cube+ dataset (White Balance)
* HDR+ dataset (Photo Retouching)

**Hardware:** 
1. Nvidia 2080Ti and Inter i7 6700
2. Nvidia 1080 Ti GPU with 11GB memory and Intel Xeon 6126

<p align="center">
  <img src="https://github.com/farkoo/AbstractVault/blob/master/IE_STAR_Arch.png" alt="STAR Architecture">
  <br>
  <em>STAR Architecture</em>
</p>

<p align="center">
  <img src="https://github.com/farkoo/AbstractVault/blob/master/IE_STAR_IE.png" alt="STAR IE Results">
  <br>
  <em>STAR IE Results</em>
</p>

<p align="center">
  <img src="https://github.com/farkoo/AbstractVault/blob/master/IE_STAR_PR.png" alt="STAR PR Results">
  <br>
  <em>STAR PR Results</em>
</p>

<br />
## DocEnTr An End-to-End Document Image Enhancement Transformer ICPR2022
**Method:** The DocEnTr model is an end-to-end image enhancement approach that restores and enhances degraded document images. It is an encoder-decoder architecture based on vision transformers. 
The input image is split into patches, which are linearly embedded, and the position information is added to them. The resulting sequence of vectors is fed to a standard Transformer encoder to obtain the latent representations. These representations are fed to another Transformer representing the decoder to obtain the decoded vector, which is linearly projected to vectors of pixels representing the output image patches. 
The encoder and decoder are inspired by the vision transformer (ViT) architecture, which uses a self-attention mechanism to give global information during every patch enhancement. The model's architecture can be modified to produce different variants, which are "Small," "Base," and "Large," depending on the number of layers, dimensions, attention heads, and parameters. 

**Dataset:**
* DIBCO 2011
* H-DIBCO 2012
* DIBCO 2017
* DIBCO 2018

**Hardware:** Not Mentioned

<p align="center">
  <img src="https://github.com/farkoo/AbstractVault/blob/master/IE_DocEnTr_Arch.png" alt="DocEnTr Architecture">
  <br>
  <em>DocEnTr Architecture</em>
</p>

<br />
## You Only Need 90K Parameters to Adapt Light A Light Weight Transformer for Image Enhancement and Exposure Correction BMVC2022

**Method:** The proposed method is called the Illumination Adaptive Transformer (IAT). It is a lightweight transformer-based model that is designed to handle sRGB images directly, without the need for raw-RGB images. The IAT model consists of two branches: a local branch and a global branch. In the local branch, the input image is mapped to a latent feature space and the transformer's attention block is replaced with depth-wise convolution for a light-weight design. In the global branch, the transformer's attention queries are used to control and adjust the global ISP-related parameters, such as the color transform matrix and gamma value. The learned queries can dynamically change under different light conditions, such as over-exposure and under-exposure. The IAT model is evaluated on several real-world and synthetic datasets, and it shows promising results in improving both the visual appearance and recognition tasks under challenging real-world light conditions.

**Dataset:**
* LOL (V1 & V2) (Image Enhancement)
* MIT-Adobe FiveK (Image Enhancement)
* Exposure Correction Dataset (Exposure Correction)
* EXDark (Low-Light Object Detection)
* ACDC (Low-Light Semantic Segmentation)
* TYOL (Various-Light Object Detection)

**Hardware:** One GeForce RTX 3090 GPU

<p align="center">
  <img src="https://github.com/farkoo/AbstractVault/blob/master/IE_IAT_Arch.png" alt="IAT Architecture">
  <br>
  <em>IAT Architecture</em>
</p>

<p align="center">
  <img src="https://github.com/farkoo/AbstractVault/blob/master/IE_IAT_IE.png" alt="IAT Image Enhancement Results">
  <br>
  <em>IAT Image Enhancement Results</em>
</p>

<p align="center">
  <img src="https://github.com/farkoo/AbstractVault/blob/master/IE_IAT_EC.png" alt="IAT Exposure Correction Results">
  <br>
  <em>IAT Exposure Correction Results</em>
</p>

<p align="center">
  <img src="https://github.com/farkoo/AbstractVault/blob/master/IE_IAT_EC.png" alt="IAT  low-light detection, low-light semantic segmentation and various light detection Results">
  <br>
  <em>IAT  low-light detection, low-light semantic segmentation and various light detection Results</em>
</p>




