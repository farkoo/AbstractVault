# Awesome Image Enhancement

| Year |	Proceeding	| Title | PDF | Code |
| :---: | :---: | :---: | :---: | :---: |
| 2023 | CVPR | [Learning a Simple Low-light Image Enhancer from Paired Low-light Instances](#learning-a-simple-low-light-image-enhancer-from-paired-low-light-instances-cvpr2023) | [Click Here](https://openaccess.thecvf.com/content/CVPR2023/papers/Fu_Learning_a_Simple_Low-Light_Image_Enhancer_From_Paired_Low-Light_Instances_CVPR_2023_paper.pdf) | [Click Here](https://github.com/zhenqifu/pairlie) |
| 2023 | CVPR | [Burstormer: Burst Image Restoration and Enhancement Transformer](#burstormer-burst-image-restoration-and-enhancement-transformer-cvpr2023) | [Click Here](https://openaccess.thecvf.com/content/CVPR2023/papers/Dudhane_Burstormer_Burst_Image_Restoration_and_Enhancement_Transformer_CVPR_2023_paper.pdf) | [Click Here](https://github.com/akshaydudhane16/Burstormer) |
| 2022 | CVPR | [SNR-aware Low-Light Image Enhancement](#snr-aware-low-light-image-enhancement-cvpr2022) | [Click Here](https://openaccess.thecvf.com/content/CVPR2022/papers/Xu_SNR-Aware_Low-Light_Image_Enhancement_CVPR_2022_paper.pdf) | [Click Here](https://github.com/dvlab-research/SNR-Aware-Low-Light-Enhance) |
| 2022 | CVPR | [MAXIM: Multi-Axis MLP for Image Processing](#maxim-multi-axis-mlp-for-image-processing-cvpr2022) | [Click Here](https://arxiv.org/pdf/2201.02973.pdf) | [Click Here](https://github.com/google-research/maxim) |
| 2022 | CVPR | [Abandoning the Bayer-Filter to See in the Dark](#abandoning-the-bayer-filter-to-see-in-the-dark-cvpr2022) | [Click Here](https://arxiv.org/pdf/2203.04042.pdf) | [Click Here](https://github.com/TCL-AILab/Abandon_Bayer-Filter_See_in_the_Dark) |
| 2022 | CVPR | [URetinex-Net: Retinex-based Deep Unfolding Network for Low-light-Image-Enhancement](#uretinex-net-retinex-based-deep-unfolding-network-for-low-light-image-enhancement-cvpr2022) | [Click Here](https://openaccess.thecvf.com/content/CVPR2022/papers/Wu_URetinex-Net_Retinex-Based_Deep_Unfolding_Network_for_Low-Light_Image_Enhancement_CVPR_2022_paper.pdf) | [Click Here](https://github.com/AndersonYong/URetinex-Net) |
| 2022 | CVPR | [Toward Fast, Flexible, and Robust Low-Light Image Enhancement](#toward-fast-flexible-and-robust-low-light-image-enhancement-cvpr2022) | [Click Here](https://arxiv.org/pdf/2204.10137.pdf) | [Click Here](https://github.com/vis-opt-group/SCI) |
| 2021 | CVPR | [Retinex-inspired Unrolling with Cooperative Prior Architecture Search for Low-light Image Enhancement](#retinex-inspired-unrolling-with-cooperative-prior-architecture-search-for-low-light-image-enhancement-cvpr2021) | [Click Here](https://arxiv.org/pdf/2012.05609.pdf) | [Click Here](https://github.com/KarelZhang/RUAS) |
| 2021 | CVPR | [Learning Temporal Consistency for Low Light Video Enhancement from Single Images](#learning-temporal-consistency-for-low-light-video-enhancement-from-single-images-cvpr2021) | [Click Here](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Learning_Temporal_Consistency_for_Low_Light_Video_Enhancement_From_Single_CVPR_2021_paper.pdf) | [Click Here](https://github.com/zkawfanx/StableLLVE) |
| 2020 | CVPR | [Zero-Reference Deep Curve Estimation for Low-Light Image Enhancement](#zero-reference-deep-curve-estimation-for-low-light-image-enhancement-cvpr2020) | [Click Here](https://openaccess.thecvf.com/content_CVPR_2020/papers/Guo_Zero-Reference_Deep_Curve_Estimation_for_Low-Light_Image_Enhancement_CVPR_2020_paper.pdf) | [Click Here](https://github.com/Li-Chongyi/Zero-DCE) |
| 2020 | CVPR | [Learning to Restore Low-Light Images via Decomposition-and-Enhancement](#learning-to-restore-low-light-images-via-decomposition-and-enhancement-cvpr2020) | [Click Here](https://openaccess.thecvf.com/content_CVPR_2020/papers/Xu_Learning_to_Restore_Low-Light_Images_via_Decomposition-and-Enhancement_CVPR_2020_paper.pdf) | [Click Here](https://drive.google.com/drive/folders/1L3RDbd3sk_TcMTrSmZXn8KLg8opjOjf0) |
| 2020 | CVPR | [From Fidelity to Perceptual Quality: A Semi-Supervised Approach for Low-Light Image Enhancement](#from-fidelity-to-perceptual-quality-a-semi-supervised-approach-for-low-light-image-enhancement-cvpr2020) | [Click Here](https://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_From_Fidelity_to_Perceptual_Quality_A_Semi-Supervised_Approach_for_Low-Light_CVPR_2020_paper.pdf) | [Click Here](https://github.com/flyywh/CVPR-2020-Semi-Low-Light) |
| 2023 | AAAI  | [Ultra-High-Definition Low-Light Image Enhancement: A Benchmark and Transformer-Based Method](#ultra-high-definition-low-light-image-enhancement-a-benchmark-and-transformer-based-method-aaai2023) | [Click Here](https://arxiv.org/pdf/2212.11548) | [Click Here](https://github.com/TaoWangzj/LLFormer) |
| 2023 | Arxiv | [Self-Reference Deep Adaptive Curve Estimation for Low-Light Image Enhancement](#self-reference-deep-adaptive-curve-estimation-for-low-light-image-enhancement) | [Click Here](https://arxiv.org/pdf/2308.08197v2.pdf) | [Click Here](https://github.com/john-venti/self-dace) |
| 2022 | ICPR | [DocEnTr: An End-to-End Document Image Enhancement Transformer](#docentr-an-end-to-end-document-image-enhancement-transformer-icpr2022) | [Click Here](https://arxiv.org/pdf/2201.10252.pdf) | [Click Here](https://github.com/dali92002/DocEnTR)
| 2022 | IEEE Trans(TPAMI ) | [Learning Enriched Features for Fast Image Restoration and Enhancement](#learning-enriched-features-for-fast-image-restoration-and-enhancement-ieee2022) | [Click Here](https://arxiv.org/pdf/2205.01649) | [Click Here](https://github.com/swz30/MIRNetv2) |
| 2022 | IEEE Trans | twin adversarial contrastive learning for underwater image enhancement and beyond | [Click Here](https://drive.google.com/file/d/1rAQVj1RamqHI0OefUTnAeqV6vNFdAm0S/view) | [Click Here](https://github.com/Jzy2017/TACL) |
| 2022 | BMVC | [You Only Need 90K Parameters to Adapt Light: A Light Weight Transformer for Image Enhancement and Exposure Correction](#you-only-need-90k-parameters-to-adapt-light-a-light-weight-transformer-for-image-enhancement-and-exposure-correction-bmvc2022) | [Click Here](https://arxiv.org/pdf/2205.14871) | [Click Here](https://github.com/cuiziteng/Illumination-Adaptive-Transformer) |
| 2021 | ICCV | [STAR: A Structure-aware Lightweight Transformer for Real-time Image Enhancement](#star-a-structure-aware-lightweight-transformer-for-real-time-image-enhancement-iccv2021) | [Click Here](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_STAR_A_Structure-Aware_Lightweight_Transformer_for_Real-Time_Image_Enhancement_ICCV_2021_paper.pdf) | [Click Here](https://github.com/zzyfd/STAR-pytorch) |
| 2021 | IEEE Trans | underwater image enhancement via medium transmission-guided multi-color space embedding | [Click Here](https://arxiv.org/pdf/2104.13015.pdf) | [Click Here](https://github.com/Li-Chongyi/Ucolor) |
| 2023 | MDPI | Low-Light Image Enhancement by Combining Transformer and Convolutional Neural Network | [Click Here](https://www.mdpi.com/2227-7390/11/7/1657/pdf?version=1680159122) | [Click Here]
| 2023 | MDPI | Unsupervised Low Light Image Enhancement Transformer Based on Dual Contrastive Learning | [Click Here](https://bmvc2022.mpi-inf.mpg.de/0373.pdf) | [Click Here](https://github.com/KaedeKK/UDCL-Transformer)
| 2022 | IEEE Trans | U-shape Transformer for Underwater Image Enhancement | [Click Here](https://arxiv.org/pdf/2111.11843) | [Click Here](https://github.com/LintaoPeng/U-shape_Transformer_for_Underwater_Image_Enhancement) |
| 2022 | CoRR | Low-Light Image and Video Enhancement: A Comprehensive Survey and Beyond | [Click Here](https://arxiv.org/pdf/2212.10772.pdf) | [Click Here] |
| 2021 | Springer | Benchmarking Low-Light Image Enhancement and Beyond | [Click Here](https://sci-hub.se/10.1007/s11263-020-01418-8) | [Click Here] |
| 2021 | IEEE Trans | Low-Light Image and Video Enhancement Using Deep Learning: A Survey | [Click Here](https://arxiv.org/pdf/2104.10729) | [Click Here] |
| 2022 | IEEE Trans | Underwater Image Enhancement via Minimal Color Loss and Locally Adaptive Contrast Enhancement | [Click Here](https://drive.google.com/file/d/1d8gLKDPoxISy6-oVc6bQJZ-sElmNDPbB/view) | [Click Here]() |
| 2020 | ECCV | Learning Enriched Features for Real Image Restoration and Enhancement | [Click Here](https://arxiv.org/pdf/2003.06792v2.pdf) | [Click Here](https://github.com/swz30/MIRNet) |

<!--- |  |  |  | [Click Here]() | [Click Here]() | -->


## Learning a Simple Low-light Image Enhancer from Paired Low-light Instances CVPR2023

**Purpose:** The purpose of this paper is to introduce PairLIE, an unsupervised approach for low-light image enhancement that learns adaptive priors from low-light image pairs. The paper aims to improve contrast and restore details for images captured in low-light conditions.

**Method:** PairLIE is an unsupervised approach for low-light image enhancement that learns adaptive priors from low-light image pairs. The method consists of two main steps:

1. Retinex Decomposition: The network is expected to generate the same clean images as the two inputs share the same image content. To achieve this, the network is imposed with the Retinex theory and makes the two reflectance components consistent.
2. Self-Supervised Mechanism: To assist the Retinex decomposition, inappropriate features in the raw image are removed with a simple self-supervised mechanism.

**Datasets:**
* SICE (part2)
* LOL (training set)

**Hardware:** Not mentioned

<p align="center">
  <img src="https://github.com/farkoo/AbstractVault/blob/master/IE_PairLIE_arch.png" alt="PairLIE Architecture">
  <br>
  <em>PairLIE Architecture</em>
</p>

<p align="center">
  <img src="https://github.com/farkoo/AbstractVault/blob/master/IE_PairLIE_result.png" alt="PairLIE Architecture">
  <br>
  <em>PairLIE Results</em>
</p>


## Burstormer Burst Image Restoration and Enhancement Transformer CVPR2023

**Method:** Burstormer is a novel transformer-based architecture for burst image restoration and enhancement. It leverages multi-scale local and non-local features to achieve improved alignment and feature fusion. The proposed method consists of two main parts: enhanced deformable alignment (EDA) and image reconstruction. EDA is a multi-scale hierarchical module that extracts noise-free local and non-local features with the burst feature attention (BFA), performs feature alignment, and refines and consolidates features with an additional interaction with the base frame via the proposed reference-based feature enrichment (RBFE) module. The input burst frames need to be properly aligned before fusing their information. Therefore, the proposed alignment module not only aligns burst features but also exchanges feature information and maintains focused communication with the reference frame through the proposed reference-based feature enrichment mechanism, which facilitates handling complex motions. Burstormer sets new state-of-the-art on several real and synthetic benchmark datasets for the task of *burst super-resolution*, *burst low-light enhancement*, and *burst denoising*.

**Dataset:**
* SyntheticBurst
* BurstSR
* SID
* Grayscale burst denoising on the dataset by [30]
* Color burst denoising on the dataset by [41]
  
**Hardware:**  Four RTX6000 GPU

<p align="center">
  <img src="https://github.com/farkoo/AbstractVault/blob/master/IE_Burstormer_Arch.png" alt="Burstormer Architecture">
  <br>
  <em>Burstormer Architecture</em>
</p>

<p align="center">
  <img src="https://github.com/farkoo/AbstractVault/blob/master/IE_Burstormer_SR.png" alt="Burstormer Super Resolution">
  <br>
  <em>Burstormer Super Resolution</em>
</p>

<p align="center">
  <img src="https://github.com/farkoo/AbstractVault/blob/master/IE_Burstormer_LLIE.png" alt="Burstormer Low-Light Image Enhancement">
  <br>
  <em>Burstormer Low-Light Image Enhancement</em>
</p>

<p align="center">
  <img src="https://github.com/farkoo/AbstractVault/blob/master/IE_Burstormer_DN.png" alt="Grayscale and Color Burst Denoising">
  <br>
  <em>Grayscale and Color Burst Denoising</em>
</p>


## SNR-Aware Low-light Image Enhancement CVPR2022

**Method:** The SNR-Aware Low-light Image Enhancement method uses a signal-to-noise-aware framework consisting of a new SNR-aware transformer design and a convolutional model to adaptively enhance low-light images in a spatial-varying manner. The method first obtains an SNR map using a simple and yet effective strategy, which estimates the Signal-to-Noise Ratio (SNR) of the input low-light image. The SNR map is then used to guide the framework to learn different enhancement operations adaptively for image regions of varying signal-to-noise ratios. In the deepest hidden layer of the framework, a new self-attention module is used to enhance pixels in a spatial-varying manner and avoid inaccurate information from regions of very low SNR. The method has been shown to achieve superior perceptual quality and consistently outperform other approaches on seven benchmarks.

- Transformer, a signal-to-noise ratio aware transformer for LLIE
  
**Dataset:**
* LOL
* SID
* SMID
* SDSD

**Hardware:** It train and test it on a PC with a 2080Ti GPU

<p align="center">
  <img src="https://github.com/farkoo/AbstractVault/blob/master/IE_SNRAT_Arch.png" alt="SNR-Aware Low-light Image Enhancement Architecture">
  <br>
  <em>SNR-Aware Low-light Image Enhancement Architecture</em>
</p>

<p align="center">
  <img src="https://github.com/farkoo/AbstractVault/blob/master/IE_SNRAT_Result.png" alt="SNR-Aware Low-light Image Enhancement Results">
  <br>
  <em>SNR-Aware Low-light Image Enhancement Results</em>
</p>


## MAXIM Multi-Axis MLP for Image Processing CVPR2022

**Method:** The proposed method in this paper is called MAXIM, which is a multi-axis MLP-based architecture for image processing tasks. MAXIM is designed to address the limitations of traditional convolutional neural networks (CNNs) in capturing global context and long-range dependencies in low-level vision tasks. It achieves this by using multi-axis MLP modules within each block to globally aggregate repeated patterns across various scales. MAXIM also incorporates self-attention mechanisms to capture long-range dependencies and efficiently mix local and global visual cues. Experimental results show that MAXIM outperforms state-of-the-art methods in various image restoration and enhancement tasks.

MAXIM is a generic architecture that can be used for a wide range of image processing tasks, including but not limited to *denoising*, *deblurring*, *deraining*, *dehazing*, and *enhancement*. 

**Dataset:**
* SIDD (Denoising)
* DND (Denoising)
* GoPro (Debluring)
* HIDE (Debluring)
* RealBlur-J (Debluring)
* RealBlur-R (Debluring)
* REDS (Debluring)
* Rain14000 (Deraining)
* Rain1800 (Deraining)
* Rain800 (Deraining)
* Rain100H (Deraining)
* Rain100L (Deraining)
* Rain1200 (Deraining)
* Rain12 (Deraining)
* Raindrop (Deraining)
* RESIDE-ITS (Dehazing)
* RESIDE-OTS (Dehazing)
* MIT-Adobe FiveK (Enhancement)
* LOL (Enhancement)

**Hardware:** Not Mentioned

<p align="center">
  <img src="https://github.com/farkoo/AbstractVault/blob/master/IE_MAXIM_Arch.png" alt="MAXIM Architecture">
  <br>
  <em>MAXIM Architecture</em>
</p>

<p align="center">
  <img src="https://github.com/farkoo/AbstractVault/blob/master/IE_MAXIM_DB.png" alt="MAXIM Debluring">
  <br>
  <em>MAXIM Debluring</em>
</p>

<p align="center">
  <img src="https://github.com/farkoo/AbstractVault/blob/master/IE_MAXIM_DN.png" alt="MAXIM Denoising">
  <br>
  <em>MAXIM Denoising</em>
</p>

<p align="center">
  <img src="https://github.com/farkoo/AbstractVault/blob/master/IE_MAXIM_DR_DH.png" alt="MAXIM Deraining and Dehazing">
  <br>
  <em>MAXIM Debluring</em>
</p>

<p align="center">
  <img src="https://github.com/farkoo/AbstractVault/blob/master/IE_MAXIM_IE.png" alt="MAXIM Enhancement">
  <br>
  <em>MAXIM Enhancement</em>
</p>

## Abandoning the Bayer-Filter to See in the Dark CVPR2022
**Method:**
The proposed method in this paper is a novel pipeline that uses deep neural networks and channel-wise attention to enhance low-light images. The method is motivated by the high light sensitivity of monochrome cameras and aims to introduce extra information beyond the raw-RGB data to achieve better enhancement and denoising visual performance with fewer artifacts and more convincing restoration.

The proposed pipeline consists of three main stages: De-Bayer-Filter simulator, Channel-wise Attention Fusion, and Post-processing. In the first stage, the De-Bayer-Filter simulator generates a monochrome raw image from the input RGB image. In the second stage, the Channel-wise Attention Fusion module fuses the monochrome raw image with the RGB image to generate a high-quality enhanced image. The channel-wise attention mechanism is used to selectively emphasize the informative channels and suppress the noisy channels. In the third stage, the Post-processing module further improves the visual quality of the enhanced image by reducing the artifacts and enhancing the details.

**Dataset:**
* SID
* Mono-Colored Raw paired dataset (MCR)
  
**Hardware:** One RTX 3090 GPU

<p align="center">
  <img src="https://github.com/farkoo/AbstractVault/blob/master/IE_DBLE_Arch.png" alt="DBLE Architecture">
  <br>
  <em>DBLE Architecture</em>
</p>

<p align="center">
  <img src="https://github.com/farkoo/AbstractVault/blob/master/IE_DBLE_Result.png" alt="DBLE Results">
  <br>
  <em>DBLE Results</em>
</p>


## URetinex-Net Retinex-based Deep Unfolding Network for Low-light-Image-Enhancement CVPR2022

**Method:** URetinex-Net is a deep learning-based method proposed for low-light image enhancement. It is based on the Retinex theory, which decomposes an image into reflectance and illumination layers. URetinex-Net adapts this decomposition process by formulating it as an implicit priors regularized model and then unfolding the update steps in the optimization into a deep neural network. 

URetinex-Net includes three learnable modules: an initialization module, an unfolding optimization module, and an illumination adjustment module. The initialization module generates the initial reflectance and illumination layers by passing a target low-light image. The unfolding optimization module then refines the reflectance and illumination layers iteratively. Finally, the illumination adjustment module outputs the enhanced normal-light version according to the user-defined ratio.

URetinex-Net has shown to be highly efficient in enhancing low-light images with successfully noise suppressing and details preserving.

**Dataset:**
* LOL
* DICE
* MEF

**Hardware:** NVIDIA Tesla V100 GPU

<p align="center">
  <img src="https://github.com/farkoo/AbstractVault/blob/master/IE_URetinex-Net_Arch.png" alt="URetinex-Net Architecture">
  <br>
  <em>URetinex-Net Architecture</em>
</p>

<p align="center">
  <img src="https://github.com/farkoo/AbstractVault/blob/master/IE_URetinex-Net_Result.png" alt="URetinex-Net Results">
  <br>
  <em>URetinex-Net Results</em>
</p>

## Toward Fast Flexible and Robust Low-Light Image Enhancement CVPR2022

**Method:** The proposed method in this paper is called Self-Calibrated Illumination (SCI) and it is a framework for low-light image enhancement. The framework consists of two main components: a basic unit and an auxiliary process. 

The basic unit is a convolutional neural network (CNN) that takes a low-light image as input and outputs an enhanced image. The auxiliary process is designed to calibrate the illumination of the input image and improve the performance of the basic unit. 

The calibration process involves estimating the illumination map of the input image and using it to adjust the image's exposure. This is done by training a separate CNN to predict the illumination map from the input image. The predicted illumination map is then used to adjust the exposure of the input image before it is fed into the basic unit. 

**Dataset:**
* MIT
* LSRW
* DARK FACE
* ACDC
* ExDark
* WIDER FACE

**Hardware:** One TITAN X GPU

## Retinex-inspired Unrolling with Cooperative Prior Architecture Search for Low-light Image Enhancement CVPR2021

**Method:** The proposed method, RUAS, first establishes a fundamental network structure based on the Retinex theory and then automatically discovers the embedded atomic prior architectures by unrolling the corresponding optimization processes. RUAS also provides a cooperative bilevel search strategy that can simultaneously discover architectures from a compact search space for both illumination estimation and noise removal. This strategy does not require any paired/unpaired supervisions during the search process. Finally, RUAS offers flexibility in searching prior architectures for different kinds of low-light scenarios. Extensive experiments show that the established enhancement networks are memory and computation efficient and can perform favorably against state-of-the-art approaches.

**Dataset:**
* MIT-Adobe 5K
* LOL
* DarkFace
* ExtremelyDarkFace

**Hardware:** Single TITAN X GPU and Intel Core i7-7700 3.60GHz CPU

<p align="center">
  <img src="https://github.com/farkoo/AbstractVault/blob/master/IE_RUAS_Result.png" alt="RUAS Results">
  <br>
  <em>RUAS Results</em>
</p>

## Learning Temporal Consistency for Low Light Video Enhancement from Single Images CVPR2021

**Method:** The proposed method in this paper is an image-based approach to low light video enhancement that addresses the problem of temporal inconsistency when using only single image data. The method uses optical flow to represent motion between video frames and embeds temporal consistency implicitly into image-based models. 

The method uses a siamese network architecture to train image pairs of original images and warped images with corresponding optical flow. By imposing consistency between output pairs, the network can be made temporally stable.

**Dataset:**

**Hardware:**


## Learning to Restore Low-Light Images via Decomposition-and-Enhancement CVPR2020

**Method:** The proposed method in this paper is a frequency-based decomposition-and-enhancement model for enhancing low-light images. The method consists of two stages. In the first stage, the model uses an Attention to Context Encoding (ACE) module to adaptively select low-frequency information for recovering the low-frequency layer and noise removal. In the second stage, the model selects high-frequency information for detail enhancement. The model also uses a Cross Domain Transformation (CDT) module to leverage multi-scale frequency-based features for noise suppression and detail enhancement in the two stages. 

The proposed method is based on the insight that low-light images suffer from two main problems: loss of details and noise. By decomposing the image into low-frequency and high-frequency layers, the model can recover the lost details in the low-frequency layer while suppressing noise. The model can then enhance the high-frequency details in the second stage. 

**Dataset:**
* sRGB

**Hardware:** An i7 4GHz CPU and a GTX 1080Ti GPU

<p align="center">
  <img src="https://github.com/farkoo/AbstractVault/blob/master/IE_DaE_Arch.png" alt="DaE Architecture">
  <br>
  <em>DaE Architecture</em>
</p>

<p align="center">
  <img src="https://github.com/farkoo/AbstractVault/blob/master/IE_DaE_Result.png" alt="DaE Results">
  <br>
  <em>DaE Results</em>
</p>

## From Fidelity to Perceptual Quality A Semi-Supervised Approach for Low-Light Image Enhancement CVPR2020

**Method:** The proposed method is a deep recursive band network (DRBN) for low-light image enhancement. It is a semi-supervised learning approach that can effectively address the visual degradation caused by under-exposure. The DRBN architecture consists of two stages. In the first stage, the network is trained on paired low/normal-light images to recover a linear band representation. This representation is then used in the second stage to bridge the gap between the restoration knowledge of paired data and the perceptual quality provided by an unpaired high-quality image dataset. The extracted band representations of the enhanced image in the first stage of DRBN are used to recursively refine the image in a coarse-to-fine manner. This approach can be trained with both paired and unpaired data, and it can extract a series of coarse-to-fine band representations that are mutually beneficial in a recursive process. Overall, the proposed method offers a powerful and flexible architecture for low-light image enhancement.

**Dataset:** 
* LOL-Real

**Hardware:** Not Mentioned 

<p align="center">
  <img src="https://github.com/farkoo/AbstractVault/blob/master/IE_DaE_Arch.png" alt="DaE Architecture">
  <br>
  <em>DaE Architecture</em>
</p>

<p align="center">
  <img src="https://github.com/farkoo/AbstractVault/blob/master/IE_DaE_Result.png" alt="DaE Results">
  <br>
  <em>DaE Results</em>
</p>

## Zero-Reference Deep Curve Estimation for Low-Light Image Enhancement CVPR2020

**Methos:** It does not require any paired or unpaired data in the training process, and it is trained without any reference image. Instead of performing image-to-image mapping, the task is reformulated as an image-specific curve estimation problem. The proposed method takes a low-light image as input and produces high-order curves as its output. These curves are then used for pixel-wise adjustment on the dynamic range of the input to obtain an enhanced image. The curve estimation is carefully formulated so that it maintains the range of the enhanced image and preserves the contrast of neighboring pixels.

**Datasets:** 360 multi-exposure sequences from the Part1 of SICE dataset

**Hardware:** One NVIDIA 2080Ti GPU

<p align="center">
  <img src="https://github.com/farkoo/AbstractVault/blob/master/IE_Zero_DCE_Arch.png" alt="Zero-DCE Architecture">
  <br>
  <em>Zero-DCE Architecture</em>
</p>

<p align="center">
  <img src="https://github.com/farkoo/AbstractVault/blob/master/IE_Zero_DCE_Result.png" alt="Zero-DCE Results">
  <br>
  <em>Zero-DCE Results</em>
</p>

## Self-Reference Deep Adaptive Curve Estimation for Low-Light Image Enhancement

**Method:** Self-DACE (Self-Reference Deep Adaptive Curve Estimation) is a 2-stage low-light image enhancement method proposed in this paper. The method is inspired by the ZeroDCE algorithm and employs a more flexible class of adaptive adjustment curves iteratively to enhance low-light images with a wider dynamic range. The first stage of the method estimates the initial enhancement curve using a self-reference mechanism, while the second stage refines the curve using a denoising scheme. The proposed method is shown to outperform existing state-of-the-art algorithms in terms of quantitative metrics on multiple datasets.

**Dataset:**
* LOL
* LSRW
* SICE
  
**Hardware:** One NVIDIA RTX 3080-TiGPU

<p align="center">
  <img src="https://github.com/farkoo/AbstractVault/blob/master/IE_Self_DICE_Arch.png" alt="Self-DICE Architecture">
  <br>
  <em>Self-DICE Architecture</em>
</p>

<p align="center">
  <img src="https://github.com/farkoo/AbstractVault/blob/master/IE_Self_DICE_Result.png" alt="Self-DICE Results">
  <br>
  <em>Self-DICE Results</em>
</p>


## Learning Enriched Features for Fast Image Restoration and Enhancement IEEE2022

**Method:** The method presented in this paper is a new architecture for fast image restoration and enhancement. The proposed architecture is based on convolutional neural networks (CNNs) and is designed to maintain spatially-precise high-resolution representations while receiving complementary contextual information from low-resolution representations. 

The architecture consists of a multi-scale residual block that processes the input image at different scales and combines the features learned at each scale to produce the final output. The multi-scale residual block is augmented with a non-local attention mechanism that helps capture contextual information from the low-resolution representations. 

The proposed architecture is evaluated on several image restoration tasks, including image denoising, super-resolution, and JPEG artifact removal. The experimental results show that the proposed approach outperforms several state-of-the-art methods on these tasks in terms of both quantitative metrics and visual quality.

**Dataset:** 
* Dual-pixel defocus deblurring (DPDD)
* DND (Denoising)
* SIDD (Denoising)
* RealSR (Super Resolution)
* LOL (Image Enhancement)
* MIT-Adobe FiveK (Image Enhancement)
  
**Hardware:** Not Mentioned

<p align="center">
  <img src="https://github.com/farkoo/AbstractVault/blob/master/IE_MIRNet-v2_Arch.png" alt="MIRNet-v2 Architecture">
  <br>
  <em>MIRNet-v2 Architecture</em>
</p>

<p align="center">
  <img src="https://github.com/farkoo/AbstractVault/blob/master/IE_MIRNet-v2_Deblurring.png" alt="MIRNet-v2 Deblurring">
  <br>
  <em>MIRNet-v2 Deblurring</em>
</p>

<p align="center">
  <img src="https://github.com/farkoo/AbstractVault/blob/master/IE_MIRNet-v2_Deblurring.png" alt="MIRNet-v2 Denoising">
  <br>
  <em>MIRNet-v2 Denoising</em>
</p>

<p align="center">
  <img src="https://github.com/farkoo/AbstractVault/blob/master/IE_MIRNet-v2_LLIE.png" alt="MIRNet-v2 Low-Light Image Enhancement">
  <br>
  <em>MIRNet-v2 Low-Light Image Enhancement</em>
</p>

<p align="center">
  <img src="https://github.com/farkoo/AbstractVault/blob/master/IE_MIRNet-v2_SR.png" alt="MIRNet-v2 Super Resolution">
  <br>
  <em>MIRNet-v2 Super Resolution</em>
</p>


## Ultra-High-Definition Low-Light Image Enhancement A Benchmark and Transformer-Based Method AAAI2023

**Method:** LLFormer is a transformer-based low-light enhancement method introduced in this PDF file. It uses axis-based multi-head self-attention and cross-layer attention fusion blocks to significantly reduce linear complexity and improve performance on low-light images. LLFormer was benchmarked against 16 representative LLIE methods, including seven traditional non-learning methods, and was found to outperform state-of-the-art methods on both a new large-scale database and existing public datasets.

**Dataset:**
* UHD-LOL4K
* UHD-LOL8K
* LOL
* MIT-Adobe FiveK

**Hardware:** Not Mentioned

<p align="center">
  <img src="https://github.com/farkoo/AbstractVault/blob/master/IE_LLFormer_Arch.png" alt="LLFormer Architecture">
  <br>
  <em>LLFormer Architecture</em>
</p>

<p align="center">
  <img src="https://github.com/farkoo/AbstractVault/blob/master/IE_LLFormer_Result.png" alt="LLFormer Results">
  <br>
  <em>LLFormer Results</em>
</p>

## STAR A Structure-aware Lightweight Transformer for Real-time Image Enhancement ICCV2021

**Method:** The article presents a method called STAR, which is a structure-aware lightweight Transformer for real-time image enhancement. It tokenizes image patches into token embeddings and explicitly learns token-wise dependencies for image patches. STAR is specialized in image enhancement tasks and can be free of stacked convolution, making it more efficient in extracting structural information. It employs a specialized two-branch design named long-short Range Transformer to ensure it can focus on capturing global contexts, thus reducing computations. Experimental results show that STAR can effectively boost the quality and efficiency of many tasks such as illumination enhancement, auto white balance, and photo retouching.

**Dataset:** 
* MIT-Adobe FiveK (Illumination Enhancement)
* WB dataset (White Balance)
* Cube+ dataset (White Balance)
* HDR+ dataset (Photo Retouching)

**Hardware:** 
1. Nvidia 2080Ti and Inter i7 6700
2. Nvidia 1080 Ti GPU with 11GB memory and Intel Xeon 6126

<p align="center">
  <img src="https://github.com/farkoo/AbstractVault/blob/master/IE_STAR_Arch.png" alt="STAR Architecture">
  <br>
  <em>STAR Architecture</em>
</p>

<p align="center">
  <img src="https://github.com/farkoo/AbstractVault/blob/master/IE_STAR_IE.png" alt="STAR IE Results">
  <br>
  <em>STAR IE Results</em>
</p>

<p align="center">
  <img src="https://github.com/farkoo/AbstractVault/blob/master/IE_STAR_PR.png" alt="STAR PR Results">
  <br>
  <em>STAR PR Results</em>
</p>


## DocEnTr An End-to-End Document Image Enhancement Transformer ICPR2022
**Method:** The DocEnTr model is an end-to-end image enhancement approach that restores and enhances degraded document images. It is an encoder-decoder architecture based on vision transformers. 
The input image is split into patches, which are linearly embedded, and the position information is added to them. The resulting sequence of vectors is fed to a standard Transformer encoder to obtain the latent representations. These representations are fed to another Transformer representing the decoder to obtain the decoded vector, which is linearly projected to vectors of pixels representing the output image patches. 
The encoder and decoder are inspired by the vision transformer (ViT) architecture, which uses a self-attention mechanism to give global information during every patch enhancement. The model's architecture can be modified to produce different variants, which are "Small," "Base," and "Large," depending on the number of layers, dimensions, attention heads, and parameters. 

**Dataset:**
* DIBCO 2011
* H-DIBCO 2012
* DIBCO 2017
* DIBCO 2018

**Hardware:** Not Mentioned

<p align="center">
  <img src="https://github.com/farkoo/AbstractVault/blob/master/IE_DocEnTr_Arch.png" alt="DocEnTr Architecture">
  <br>
  <em>DocEnTr Architecture</em>
</p>


## You Only Need 90K Parameters to Adapt Light A Light Weight Transformer for Image Enhancement and Exposure Correction BMVC2022

**Method:** The proposed method is called the Illumination Adaptive Transformer (IAT). It is a lightweight transformer-based model that is designed to handle sRGB images directly, without the need for raw-RGB images. The IAT model consists of two branches: a local branch and a global branch. In the local branch, the input image is mapped to a latent feature space and the transformer's attention block is replaced with depth-wise convolution for a light-weight design. In the global branch, the transformer's attention queries are used to control and adjust the global ISP-related parameters, such as the color transform matrix and gamma value. The learned queries can dynamically change under different light conditions, such as over-exposure and under-exposure. The IAT model is evaluated on several real-world and synthetic datasets, and it shows promising results in improving both the visual appearance and recognition tasks under challenging real-world light conditions.

**Dataset:**
* LOL (V1 & V2) (Image Enhancement)
* MIT-Adobe FiveK (Image Enhancement)
* Exposure Correction Dataset (Exposure Correction)
* EXDark (Low-Light Object Detection)
* ACDC (Low-Light Semantic Segmentation)
* TYOL (Various-Light Object Detection)

**Hardware:** One GeForce RTX 3090 GPU

<p align="center">
  <img src="https://github.com/farkoo/AbstractVault/blob/master/IE_IAT_Arch.png" alt="IAT Architecture">
  <br>
  <em>IAT Architecture</em>
</p>

<p align="center">
  <img src="https://github.com/farkoo/AbstractVault/blob/master/IE_IAT_IE.png" alt="IAT Image Enhancement Results">
  <br>
  <em>IAT Image Enhancement Results</em>
</p>

<p align="center">
  <img src="https://github.com/farkoo/AbstractVault/blob/master/IE_IAT_EC.png" alt="IAT Exposure Correction Results">
  <br>
  <em>IAT Exposure Correction Results</em>
</p>

<p align="center">
  <img src="https://github.com/farkoo/AbstractVault/blob/master/IE_IAT_EC.png" alt="IAT  low-light detection, low-light semantic segmentation and various light detection Results">
  <br>
  <em>IAT  low-light detection, low-light semantic segmentation and various light detection Results</em>
</p>


* Histogram Equalization and Adaptive Histogram Equalization (AHE)
* Contrast Enhancement Techniques
* Image Fusion
* Dehazing Techniques
* Noise Reduction
* Retinex-based Methods
* Super-Resolution Techniques
* Color Correction
* Adaptive Filtering Techniques

